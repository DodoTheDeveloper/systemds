#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------


# INPUT PARAMETERS:
# -----------------------------------------------------------------------------------------------------
# NAME           TYPE           DEFAULT  MEANING
# -----------------------------------------------------------------------------------------------------
# xTrain         Matrix[Double]  ---     Trainings data x used to score the hyperparameter sets.
# y              Matrix[Double]  ---     Trainings targets used to score the hyperparameter sets.
# params         Matrix[String]  ---     Name of the hyper parameters to optimize.
# paramValues    Matrix[Double]  ---     Values of the hyper parameters to optimize.
# objective      String          ---     The objective function to train a model with a set of hyperparameters.
# predictive     String          ---     The predicitve function used to calculate the score.
# acquisition    String          ---     Name of the acquisition function to maximize.
# acquParams     List[Unknown]   ---     List of params to apply to the acquisition function.
# kernel         String          ---     Kernelfunction to use.
# kernelParams   List[Unknown]   ---     List of params to apply to the kernel.
# iterations     Integer         ---     Number of training interations.
# randomSamples  Integer         0       Number of randome samples used to initialize the GaussianP.
# minimize       Boolean         TRUE    Returns HP set with min score if true.
# verbose        Boolean         TRUE    Prints additional information if true.
#

m_bayesianOptimisation = function(Matrix[Double] xTrain, Matrix[Double] yTrain, List[String] params, List[Unknown] paramValues, String objective, String predictive, String acquisition, List[Unknown] acquParams, String kernel, List[Unknown] kernelParams, Integer iterations, Integer randomSamples = 0, Boolean minimize = TRUE, Boolean verbose = TRUE)
  return (Frame[Unknown] opt)
{
  numOfParams = length(params);
  HP = getMatrixOfParamCombinations(params, paramValues, verbose);
  indexes = getIndexes(nrow(HP));

  [scores, usedHPIndexes] = getInitScoreAndHyperParamIndexes(
      xTrain
    , yTrain
    , objective
    , predictive
    , HP
    , numOfParams
    , randomSamples
    , verbose
  );

  [means, variances] = updateModel(indexes, scores, kernel, kernelParams);

  for (i in 1: iterations) {

    # use aquisition function to get index of next hyperparameter set to try.
    aArgs = concatArgsList(list(means, variances), acquParams);
    nextHPSetIdx = as.scalar(eval(acquisition, aArgs)); # Altough double is returned print throw error not being able to print a matrix without toString.
    nextHPSet = HP[nextHPSetIdx];

    # eval expensive objective function with hyperparametsers.
    oArgs = concatArgsMat(list(xTrain, yTrain), nextHPSet);
    oResult = eval(objective, oArgs);


    # score result
    pArgs = list(xTrain, yTrain, oResult);
    pResult = as.scalar(eval(predictive, pArgs));
    scores[nextHPSetIdx] = pResult;

    # update model
    [means, variances] = updateModel(indexes, scores, kernel, kernelParams);

    print("\nIteration: " + i + "\nNext params: " + toString(nextHPSet) + "\n Predictive: " + toString(pResult));
  }

  opt = getFinalHPSet(HP, scores, minimize);
  print("Done");
  print("\nFinal scores:\n" + toString(scores) + "\nOptimal parameters after " + iterations + ":\n" + toString(opt));
}

getMatrixOfParamCombinations = function(List[String] params, list[Unknown] paramValues, Boolean verbose)
return (Matrix[Double] HP)
{
  # Step 0) preparation of parameters, lengths, and values in convenient form
  numParams = length(params);
  paramLens = matrix(0, numParams, 1);
  for( j in 1:numParams ) {
    vect = as.matrix(paramValues[j,1]);
    paramLens[j,1] = nrow(vect);
  }
  paramVals = matrix(0, numParams, max(paramLens));
  for( j in 1:numParams ) {
    vect = as.matrix(paramValues[j,1]);
    paramVals[j,1:nrow(vect)] = t(vect);
  }
  cumLens = rev(cumprod(rev(paramLens))/rev(paramLens));
  numConfigs = prod(paramLens);

  # Step 1) materialize hyper-parameter combinations
  # (simplify debugging and compared to compute negligible)
  HP = matrix(0, numConfigs, numParams);


  parfor( i in 1:nrow(HP) ) {
    for( j in 1:numParams )
      HP[i,j] = paramVals[j,as.scalar(((i-1)/cumLens[j,1])%%paramLens[j,1]+1)];
  }

  if( verbose )
    print("BayesianOptimization: Hyper-parameter combinations: \n"+toString(HP));
}

getInitScoreAndHyperParamIndexes = function(Matrix[Double] xTrain, Matrix[Double] yTrain, String objective, String predictive, Matrix[Double] HP, Integer numOfParams, Integer numOfSamples, Boolean verbose = TRUE)
return(Matrix[Double] scores, List[Integer] usedHPIndexes) {
  samples = sample(nrow(HP), numOfSamples);
  scores = matrix(0, nrow(HP), 1);
  usedHPs = matrix(0, numOfSamples, numOfParams);
  usedHPIndexes = list();

  for ( sampleIdx in 1: numOfSamples) {
    rndNum = as.scalar(samples[sampleIdx,1]);
    usedHPIndexes = append(usedHPIndexes, rndNum);
    HPSet = HP[rndNum,];

    # calc objective
    tArgs = list(xTrain, yTrain);
    for ( paramIdx in 1: numOfParams) {
        print(sampleIdx + " " + paramIdx);
        tArgs = append(tArgs, as.scalar(HPSet[1, paramIdx]));
    }
    usedHPs[sampleIdx,] = HPSet[1,];
    objResult = eval(objective, tArgs);

    # calc predictive / score
    pArgs = list(xTrain, yTrain, objResult);
    predResult = eval(predictive, pArgs);
    scores[rndNum] = predResult;
  }

  if (verbose) {
    print("\nInit model with randome samples:");
    print("\nscores:\n" + toString(scores) +
          "\nhyperparameters:\n" + toString(usedHPs) +
          "\nhyperparameter Indexes:\n" + toString(usedHPIndexes));
  }
}

getIndexes = function(Integer numOfSamples)
return (Matrix[Double] indexes)
{
  indexes = matrix(0, numOfSamples, 1);
  for (i in 1:numOfSamples) {
    indexes[i,1] = i;
  }
}

updateModel = function(Matrix[Double] indexes, Matrix[Double] scores, String kernel, List[Unknown] kernelParams)
return (Matrix[Double] means, Matrix[Double] variances)
{
  numOfSamples = nrow(indexes);
  means = matrix(0, numOfSamples, 1);
  variances = matrix(0, numOfSamples, 1);

  [firstMean, firstVariance] = getMeanAndVarianceOfFirstIndex(as.scalar(scores[1,1]));
  means[1,1] = firstMean;
  variances[1,1] = firstVariance;

  for (i in 2:numOfSamples) {
    [mean, variance] = getMeanAndVariance(
        indexes[1:i-1,1]
      , as.matrix(i)
      , scores[1:i-1,1]
      , kernel
      , kernelParams
    );
    means[i,1] = mean;
    variances[i,1] = variance;
  }

  print("\nMeans:\n" + toString(means) + "\nVariances\n" + toString(variances));
}

getMeanAndVarianceOfFirstIndex = function(Double score)
return (Double mean, Double variance)
{
  if (score == 0) { # first index not sampled
    mean = 0;
    variance = 1;
  } else { # first index sampled
    mean = score;
    variance = 0;
  }
}

getMeanAndVariance = function(Matrix[Double] indexes, Matrix[Double] xNew, Matrix[Double] scores, String kernel, List[Unknown] kernelParams)
return (Double mean, Double variance)
{
  KArgs = concatArgsList(list(indexes, indexes), kernelParams);
  K = eval(kernel, KArgs);

  KsArgs = concatArgsList(list(indexes, xNew), kernelParams);
  Ks = eval(kernel, KsArgs);

  KssArgs = concatArgsList(list(xNew, xNew), kernelParams);
  Kss = eval(kernel, KssArgs);

  K_inv = inv(K);

  #TODO: This throws an error. When scores is a vector with only zeroes [0.000, 0.000, 0.000], than the matrix multiplication seems to not correctly count the rows. nrow(scores) shows the correct result. This only happens when scores is provided as a argument to the function as a new matrix created in the function scope works without an issue.
  #mean = as.scalar(t(Ks) %*% K_inv %*% scores);

  w_scores = getWorkaroundMat(scores);
  mean = as.scalar(t(Ks)%*%K_inv%*%w_scores);
  variance = as.scalar(Kss - t(Ks) %*% K_inv %*% Ks);

  print("\n Mean, Variance:\n" + mean + "\n" + variance);
}

# Returns a copy of the input matrix. This workaround prevents an issue where the matrix optimizer would not correctly count the dimension of the input matrix.
getWorkaroundMat = function(Matrix[Double] scores)
return (Matrix[Double] wScores) 
{
  wScores = matrix(0, nrow(scores), 1);
  for (i in 1:nrow(scores)) {
    wScores[i,1] = scores[i,1];
  }
}

concatArgsList = function(List[Unknown] a, List[Unknown] b)
return (List[Unknown] result)
{
  result = a;
  if ( length(b) != 0 ) {
    for(i in 1:length(b)) {
      result = append(result, b[i]);
    }
  }
}

concatArgsMat = function(List[Unknown] a, Matrix[Double] b)
return (List[Unknown] result)
{
  result = a;
  for(i in 1:ncol(b)) {
    result = append(result, as.scalar(b[1,i]));
  }
}

getFinalHPSet = function(Matrix[Double] HP, Matrix[Double] scores, Boolean minimize)
return (Frame[Unknown] opt)
{
  if (minimize) {
    idx = as.scalar(rowIndexMin(t(scores)));
  } else {
    idx = as.scalar(rowIndexMax(t(scores)));
  }
  opt = as.frame(HP[idx]);

}
