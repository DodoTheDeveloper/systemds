#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

source("./scripts/staging/bayesian_optimisation/bayesianOptimisation.dml") as bayOpt;

gaussianKernel = function(Matrix[Double] X1, Matrix[Double] X2, Double width = 1)
return (Matrix[Double] result)
{
  square_distance = (X1 - t(X2))^2;
  result = exp((-1)*square_distance/(2 * width^2));
}


l2norm = function(Matrix[Double] X, Matrix[Double] y, Matrix[Double] B) 
return (Matrix[Double] loss) { print("called train function");
  loss = as.matrix(sum((y - X%*%B)^2));
}

lowerConfidenceBound = function(Matrix[Double] means, Matrix[Double] variances, Double beta)
return (Double index)
{
  alphas = means - beta * variances;
  index = as.scalar(rowIndexMin(t(alphas)));
}

params = list("icpt", "reg", "tol", "maxi", "verbose");
paramValues = list(as.matrix(0), 10^seq(0,-4), 10^seq(-6,-12), 10^seq(3,6), as.matrix(1));

N = 200;
X = read($1);
y = read($2);

xTrain = X[1:N,];
yTrain = y[1:N,];

Xtest = X[(N+1):nrow(X),];
ytest = y[(N+1):nrow(X),];

opt = bayOpt::m_bayesianOptimisation(
    xTrain = xTrain
  , yTrain = yTrain
  , params = params
  , paramValues = paramValues
  , objective = "lm"
  , predictive = "l2norm"
  , acquisition = "lowerConfidenceBound"
  , acquParams = list(80) # beta
  , kernel = "gaussianKernel"
  , kernelParams = list(1) # stddev
  , iterations = 40
  , randomSamples = 10
  , minimize = TRUE
  , verbose = TRUE);

B1 = lm(
  X=xTrain,
  y=yTrain,
  icpt = as.scalar(opt[1,1]),
  reg = as.scalar(opt[1,2]),
  tol = as.scalar(opt[1,3]),
  maxi = as.scalar(opt[1,4]),
  verbose = FALSE
);

B2 = lm(X=xTrain, y=yTrain, verbose=FALSE);

l1 = l2norm(Xtest, ytest, B1);
l2 = l2norm(Xtest, ytest, B2);

print("\nDefault Params: " + as.scalar(l2) + "\nBayes: " + as.scalar(l1));

R = as.scalar(l1 < l2);
write(R, $3);

