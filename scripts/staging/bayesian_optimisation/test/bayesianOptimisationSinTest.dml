# Test
source("./scripts/staging/bayesian_optimisation/bayesianOptimisation.dml") as bayOpt;

kernel_gaus = function(Matrix[Double] X1, Matrix[Double] X2, Double variance)
  return (Matrix[Double] result)
{
    #TODO: What to do about multiple dimensional matrix?
    square_distance = abs((X1 - t(X2))%*%t(X1-t(X2)));
    result = exp((-1)*square_distance/(2 * variance));
}

l2norm = function(Matrix[Double] X, Matrix[Double] y, Matrix[Double] B)
  return (Matrix[Double] loss)
{
    print("called train function");
    loss = as.matrix(sum((y - X%*%B)^2));
}

acquisition = function(Matrix[Double] X, Matrix[Double] y) 
return (Double result) {
    print("called aquisition");
    result = 1;
}

params = list("reg", "tol");
paramValues = list(10^seq(0,-4), 10^seq(-6,-12), 10^seq(1,3));

xTest = seq(0,10,0.1);
trainFunctin = "sin";
xTrain = xTest[1:50];
yTrain = sin(xTrain);

OptiHyperParams = bayOpt::m_bayesianOptimisation(
      X = xTrain
    , y = yTrain
    , fTrain = "sin"
    , fAqu = "l2norm"
    , kernel = "kernel_gaus"
    , params = params
    , paramValues = paramValues
    , iterations = 20
    , scale = 1
    , verbose = TRUE
);

write(OptiHyperParams, $1);

