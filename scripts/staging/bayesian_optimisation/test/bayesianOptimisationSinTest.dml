# Test
source("./scripts/staging/bayesian_optimisation/bayesianOptimisation.dml") as bayOpt;

kernel_gaus_1d = function(Matrix[Double] X1, Matrix[Double] X2, Double variance)
  return (Matrix[Double] result)
{
    #TODO: What to do about multiple dimensional matrix?
    square_distance = abs((X1 - t(X2))%*%t(X1-t(X2)));
    result = exp((-1)*square_distance/(2 * variance));
}

squared_exponential = function(Matrix[Double] X1, Matrix[Double] X2)
  return (Matrix[Double] result)
{
    lenX = nrow(X1);
    result = matrix(1, lenX, lenX);
    for(i in 1:lenX) { #TODO: When using parfor, results are not getting merged, whole result is full of 1s.
        for(j in 1:lenX) {
            square_distance = abs(X1[i] - X2[j])^2;
            result[i,j] = exp(square_distance/(-2));
            #print(toString(a));

        }
    }
    #print(toString(result));
}


l2norm = function(Matrix[Double] X, Matrix[Double] y, Matrix[Double] B)
  return (Matrix[Double] loss)
{
    print("called train function");
    loss = as.matrix(sum((y - X%*%B)^2));
}

acquisition = function(Matrix[Double] X, Matrix[Double] y) 
return (Double result) {
    print("called aquisition");
    result = 1;
}

params = list("reg", "tol");
paramValues = list(10^seq(0,-4), 10^seq(-6,-12), 10^seq(1,3));

xTest = seq(0,6,0.15);
trainFunctin = "sin";
xTrain = xTest[1:20];
yTrain = sin(xTrain);


OptiHyperParams = bayOpt::m_bayesianOptimisation(
      X = xTrain
    , y = yTrain
    , fTrain = "sin"
    , params = params
    , paramValues = paramValues
    , fAqu = "l2norm"
    , kernel = "squared_exponential"
    #, kernel = "kernel_gaus_1d"
    , iterations = 1
    , verbose = TRUE
);

write(OptiHyperParams, $1);

